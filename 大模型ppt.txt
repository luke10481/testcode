https://deepseek.csdn.net/67cd5e1d6670175f9932c61c.html#devmenu8
http://www.anquan419.com/news/21/2720.html

大模型相关术语 
多模态
Ø 文本、图片、音频、视频
l AI工具（国内）
Ø DeepSeek、 豆包、Kimi、腾讯元宝、智谱清言、
通义千问、秘塔搜索、微信搜索...
 l 通用模型
Ø 大语言模型（LLM，Large Language Model）
Ø 视觉模型（图片、视频）
Ø 音频模型
Ø 多模态模型
Ø ……
 l 行业模型（垂直模型、垂类模型）
Ø 教育、医疗、金融等

大模型的前世今生
•人工智能：让机器具备动物智能，人类智能，非人类智能（超人类智能）
•运算推理：规则核心；自动化
•知识工程：知识核心；知识库+推理机
•机器学习：学习核心；数据智能（统计学习方法，数据建模）
•常规机器学习方法：逻辑回归，决策森林，支持向量机，马尔科夫链，…..
 •人工神经网络：与人脑最大的共同点是名字，机制和架构并不一样
•传统神经网络：霍普菲尔德网络，玻尔兹曼机，…..
 •深度神经网络：深度学习
•传统网络架构：DBN，CNN，RNN，ResNet，Inception，……
 •Transformer架构：可以并行矩阵计算（GPU），核心是注意力机制（Attention）
•编码器（BERT）：多数embedding模型，Ernie早期版本，…….
 •混合网络：T5、GLM
 •解码器（GPT）：大语言模型（LLM），也是传统的多模态模型的核心
•生成式人工智能（GenAI）：AIGC
 •DeepSeek、Qwen、GLM、Step、MiniMax、hunyuan、kimi、……
 •OpenAI GPT（ChatGPT）、Claude、Llama、Grok、……
 •Diffusion架构：主要用于视觉模型（比如Stable Diffusion、DALLE），现在也开始尝试用于语言模型
•Diffusion+Transformer架构：例如Sora的DiT（加入Diffusion的视觉模型），部分新的多模态模型架构

生成模型与推理大模型的对比
比较项OpenAI GPT-4o（生成模型）OpenAI o1（推理模型）
模型定位
专注于通用自然语言处理和多模态能力，适合日常对
话、内容生成、翻译以及图文、音频、视频等信息处
理、生成、对话等。
侧重于复杂推理与逻辑能力，擅长数学、编程和自然语言推理任
务，适合高难度问题求解和专业领域应用。一般是在生成模型的
基础上通过RL方法强化CoT能力而来
推理能力在日常语言任务中表现均衡，但在复杂逻辑推理（如
数学题求解）上准确率较低。在复杂推理任务表现卓越，尤其擅长数学、代码推理任务。
多模态支持支持文本、图像、音频乃至视频输入，可处理多种模
态信息。当前主要支持文本输入，不具备图像处理等多模态能力。
应用场景
适合广泛通用任务，如对话、内容生成、多模态信息
处理以及多种语言相互翻译和交流；面向大众市场和
商业应用。
适合需要高精度推理和逻辑分析的专业任务，如数学竞赛、编程
问题和科学研究；在思路清晰度要求高的场景具有明显优势，比
如采访大纲、方案梳理。
用户交互体
验
提供流畅的实时对话体验，支持多种输入模态；用户
界面友好，适合大众使用。可自主链式思考，不需要太多的过程指令，整体交互节奏较慢。


DeepSeek最新的生成模型和推理模型版本对比
比较方面生成模型（V3）推理模型（R1）
设计初衷
想要在各种自然语言处理的任务中都
能表现好，更通用
重点是为了搞定复杂的推理情况，比如
深度的逻辑分析和解决问题
性能展现
在数学题、多语言任务还有编码任务
里表现不错，像Cmath能得90.7分，
Human Eval编码任务通过率是65.2%
在需要逻辑思考的测试里很棒，比如
DROP任务F1分数能达到92.2%，AIME 
2024的通过率是79.8%
应用的范围
适合大规模的自然语言处理工作，像
对话式AI、多语言翻译还有内容生成
等等，能给企业提供高效的AI方案，
满足好多领域的需求
适合学术研究、解决问题的应用和决策
支持系统等需要深度推理的任务，也能
拿来当教育工具，帮学生锻炼逻辑思维

为什么火：能力突破、开源、低成本、国产化
基础能力：进入推理模型阶段，跻身全球第一梯队
推理能力跃升：DeepSeek大模型核心技术突破，实现复杂推理任务的精准处理与
高效执行，覆盖多模态场景应用。
国际竞争力对标：模型综合性能跃居全球第一梯队，技术指标与国际顶尖水平（如
GPT系列、Claude等）直接对标，奠定国产大模型的行业标杆地位。
DeepSeek以“推理能力+第一梯队性能”
为核心基础，叠加：开源开放、超低成本、
核心加分项：开源、低成本、国产化
• 开源：技术共享，生态共建
全量开源训练代码、数据清洗工具及微调框架，开发者可快速构建教育、金融、医
疗等垂直领域应用，推动社区协同创新。
• 低成本：普惠企业级AI应用
做了大量的模型架构优化和系统工程优化。
训练成本仅$557w ：显著低于行业同类模型，打破高价壁垒。
推理成本降低83%：千亿参数模型适配中小企业需求，加速商业化落地。
• 国产化：技术自主，缩短差距
将国产模型与美国的代际差距从3-5年缩短至3-5个月，突破“卡脖子”技术瓶颈。
国产自主研发三大优势，不仅实现技术代际
跨越，更推动AI技术普惠化与国产化生态繁
荣，成为全球大模型赛道的重要领跑者。
构建多行业专属模型矩阵，全面支持国内产业智能化升级。

DeepSeek的算法和算力突破
• DeepSeek R1达到了跟o1相当、或者至少接近的推理能力，且将推理过程可视化
• 它做到这个水平只用到少得多的资源，所以价格十分便宜
• 它是完全开源的并且还发布论文，详细介绍了训练中所有的步骤和窍门
• DeepSeek深度求索公司是一家纯粹的中国公司
Deepseek官网地址：
http://ai.com
 https://chat.deepseek.com
 DeepSeek-R1 训练技术全部公开，论文链接：
https://github.com/deepseek-ai/DeepSeek
R1/blob/main/DeepSeek_R1.pdf
混合专家
MOE
多头潜在注意力
MLA
强化学习
GRPO
 19 
直接硬件编程
PTX
混合精度训练
FP8
多Token预测
MTP
通讯优化
DualPipe
并行训练框架
HAI
测试时计算
TTC

对AI行业的重大影响
01
打破垄断
DeepSeek-R1以低成本和开源
02
价格下调
DeepSeek-R1的API定价仅为
特性打破以往头部企业巨头割
据局面
行业均价的1/10,推动了中小型
企业低成本接入AI,对行业产生
了积极影响
03
推动创新
DeepSeek-R1促使行业开始从
“唯规模论”转向更加注重
“性价比”和“高效能”方向

DeepSeek私有化部署
模型的私有化部署的方式：
• Ollama部署: 个人本地部署【推荐】，方便快速，适用于蒸馏模型
• vLLM部署：生产、开发、垂直领域私有化部署，精度可控，更专业
•其它
Model Base Model特点
DeepSeek-R1-Distill-Qwen-1.5B Qwen2.5-Math-1.5B
蒸馏模型，能力稍弱
实际上是增加了推理能力的Qwen模型和Llama模型 
DeepSeek-R1-Distill-Qwen-7B Qwen2.5-Math-7B
 DeepSeek-R1-Distill-Llama-8B Llama-3.1-8B
 DeepSeek-R1-Distill-Qwen-14B Qwen2.5-14B
 DeepSeek-R1-Distill-Qwen-32B Qwen2.5-32B
 DeepSeek-R1-Distill-Llama-70B Llama-3.3-70B-Instruct
 DeepSeek-R1-671B DeepSeek-V3-Base满血版，能力最强

GPT工作原理-1
 G
 P
 T
 Generative
（生成式）
Pre-trained
（预训练）
Transformer
（变换器）
LLM:Large Language Model
 1. 收到提示词
示例:“今天天气不错，我决定”
 2.将输入拆分为token
 [“今”, “天”, “天”, “气”, “不”, 
“错”, “，”, “我”, “决”, “定”]
 3.采用Transformer架构处理token
 • 理解token之间的关系
• 识别提示词的整体含义
4.基于上下文预测下一个token
 • 为可能的单词分配概率分数
• 示例:{“去”:0.7. “停":0.2,“站":0.1}
 5.根据概率分数选择标记
示例:“去”
概率预测+文字接龙
自回归：重复步骤4和
步骤5直到形成完整的
句子
示例:今天天气不错，我
决定去公园

GPT工作原理-2
预训练
（自监督）
监督微调
人类反馈
强化学习
阶段1：模型训练
大模型工作过程
上下文
+
训练知识
接收输入
处理输入
进行推理
生成输出
阶段2：推理

GPT工作原理-3
数据来源
说明
维基百科
图书
在线百科,严谨
经典为主的古登堡计划和自助出版 
平台Smashwords等
杂志期刊
链接
论文：ArXiv等
WebText,Reddit
 Common Crawl
 GitHub
开源项目,爬取互联网所有数据
程序员聚集地
• 700 多GB,约有19万套四大名著 
的阅读量
合计
• 5 000亿左右的token数量。（13
万亿token:gpt4)
 • 100个标记大约等于75个英语单
词
典型的新技能学习曲线：规模到达临界点之后才会迅速增长
模型参数：1.8万亿参数（GPT-4)
 GPT-4o
上下文窗口大小：8192个token(标记)

生成模型的优势与劣势
优势劣势
n 语言理解和生成能力
n 世界知识能力
n 一定的推理能力
vs
 n 幻觉（生成错误答案）
n 知识库有限
n 上下文窗口限制

推理模型（DeepSeek-R1)工作原理
思维链
（Chain of Thought）
让模型进行慢思考
蒸馏
（Distillation）
在不损失能力的情况下缩小模型
强化学习
（Reinforcement Learning）
让模型自我探索和训练
l DeepSeek R1论文：https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf
 l DeepSeek R1论文图解：https://zhuanlan.zhihu.com/p/20844750193

1.DeepSeek提示词技巧-真诚+直接
传统
你现在是一个新能源汽车的市场研究
分析师，这里有一份调研报告总结需
要写成周报，请按周报的格式帮我完
成并进行润色，不少于500字。
DeepSeek
帮我把这份报告包装一下，我要写成周报给
老板看，老板很看重数据。

2.DeepSeek提示词技巧-通用公式
我要（做）**，要给**用，希望达到**效果，但担心**问题
例如：我要做一个从北京到日本的旅游攻略，要
给爸妈用，希望让他们在日本
开心的玩20天，但我担心他们玩
的累，腿和腰不太好

3.DeepSeek提示词技巧-说人话
适合场景：科研，了解新事物
了避免DeepSeek的回答过于官方、专业，可以尝试这三个字“
说人话”
你问：什么是“波粒二象性”，DeepSeek大概率会给出专业且看不懂的回答，和百度百科差不多。但如
果给ta一句“
说人话”，ta就会生动形象的做一些举例
说人话

4.DeepSeek提示词技巧-反向PUA
 DeepSeek有一套自己的思维链，也就是ta自带的思考逻辑，那么如果你想要DeepSeek更卖力给你搬砖，
就需要你运用“反向PUA”
 “请你列出10个反对理由再给方案”
 “如果你是老板，你会怎样批评这个方案？”
 “这个回答你满意吗？请你把回答复盘至少10轮”

5.DeepSeek提示词技巧-善于模仿
42
如果你想写一篇文案，用提示词约束，可能效果一般般，但如果你给一篇文章模仿或者让ta
模仿谁的语气，DeepSeek大概率会写到你的心趴上。

6.DeepSeek提示词技巧-擅长锐评
DeepSeek自带情商，各种语气也能完美拿捏！

 ”__________，笑死“句式，触发DeepSeek的毒舌属性
6.DeepSeek提示词技巧-擅长锐评

7.DeepSeek提示词技巧-激发深度思考
推理
复盘
深度思考
批判
Normal Model
在提示词结尾加入：
“在你的回答中，同时加入你的批判性思考”
 “在你回答之前，先自己复盘100遍“

DeepSeek官方提示词解读
• 代码类
• 内容分类
• 结构化输出
• 角色扮演（人设、情景）
• 创作类
• 翻译类
• 提示词生成
清晰表达，提示工作流程
目标、能力、知识储备、使用说明
结构化要求+示例
描述角色性格特征、指定输出情景
风格、要求、主题
“信”、“达”、“雅”
生成提示词的提示词
官方文档链接：
https://api-docs.deepseek.com/zh-cn/prompt-library

DeepSeek-R1 应用场景
l推理密集型任务
Ø 编程任务中的代码生成、算法设计，媲美Claude 3.5 Sonet
 Ø 数学问题求解、科学推理和逻辑分析等需要复杂推理的场景。
l教育与知识应用
Ø 可用于解决教育领域的问题，支持知识理解与解答。
Ø 可用于科研任务的实验设计、数据分析和论文撰写。
l文档分析与长上下文理解
Ø 适合处理需要深入文档分析和理解长上下文的任务，例如复杂信息提取与整合。
l开放领域问答与写作
Ø 在内容生成、问题回答以及创造性写作中具有广泛应用，例如生成高质量文本或进行内容编辑

用例生成

